{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6557b8fe",
   "metadata": {},
   "source": [
    "<img src=\"./images/DLI_Header.png\" width=400/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7aaf77d",
   "metadata": {},
   "source": [
    "# Reconstructing Outdoor Environments for Physical AI Simulation with 3D Gaussian Splatting in NVIDIA Isaac Sim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a806ec0",
   "metadata": {},
   "source": [
    "**Table of Contents**\n",
    "<br>\n",
    "In this notebook we demonstrate how to reconstruct an outdoor scene. This includes the following sections:\n",
    "\n",
    "[Part 1: Creating a Digital Twin with Gaussian Splatting](#part-1-creating-a-digital-twin-with-gaussian-splatting)<br>\n",
    "    &emsp;&emsp;1. [Gather Images](#gather-images)<br>\n",
    "    &emsp;&emsp;2. [Run Structure from Motion (COLMAP)](#run-structure-from-motion-colmap)<br>\n",
    "    &emsp;&emsp;3. [fVDB Reality Capture](#fvdb-reality-capture)<br>\n",
    "    &emsp;&emsp;&emsp;&emsp;a) [Viewing a Sfm Scene](#viewing-a-sfm-scene) <br>\n",
    "    &emsp;&emsp;&emsp;&emsp;b) [Reconstructing a Scene with Gaussian Splatting](#reconstructing-a-scene-with-gaussian-splatting) <br>\n",
    "    &emsp;&emsp;4. [Editing Gaussian Splats](#editing-gaussian-splats)<br>\n",
    "    &emsp;&emsp;5. [Create Isaac Sim Ready Files](#create-isaac-sim-ready-files)<br>\n",
    "    &emsp;&emsp;&emsp;&emsp;a) [Convert to a Mesh](#convert-to-a-mesh) <br>\n",
    "    &emsp;&emsp;&emsp;&emsp;b) [Cropping and Converting](#cropping-and-converting) <br>\n",
    "[Part 2: Creating an Isaac Sim Scene](#Part-2-creating-an-isaac-sim-scene)<br>\n",
    "    &emsp;&emsp;6. [Running Isaac Sim](#running-isaac-sim)<br>\n",
    "    &emsp;&emsp;7. [Import the Assets](#import-the-assets)<br>\n",
    "    &emsp;&emsp;8. [Scene Setup](#scene-setup)<br>\n",
    "    &emsp;&emsp;9. [Optional: Splat Color Editing](#optional-splat-color-editing)<br>\n",
    "    &emsp;&emsp;10. [Save Scene](#save-scene)<br>\n",
    "    &emsp;&emsp;11. [Isaac Lab and Robot Locomotion](#isaac-lab-and-robot-locomotion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2db670",
   "metadata": {},
   "source": [
    "## Overview\n",
    "In this lab we will learn to how to reconstruct an outdoor scene to test robots using NVIDIA fVDB framework and NVIDIA Omniverse NuRec rendering in Isaac Sim. We will walk through core reconstruction and rendering technologies, with a step-by-step workflow for simulating an entire outdoor environment for testing any robot. We will learn how to position images using structure from motion, train a 3D Gaussian splatting scene, extract 3D mesh, and convert to USD for simulation in Isaac Sim. This lab is split into two parts. In [Part 1](#part-1-creating-a-digital-twin-with-gaussian-splatting), we will learn how to gather good data, use a SfM tool, and train a Gaussian splatting scene. In [Part 2](#Part-2-creating-an-isaac-sim-scene), we will switch focus to Isaac Sim. We will import the files from [Part 1](#part-1-creating-a-digital-twin-with-gaussian-splatting) into Isaac Sim and use Isaac Lab to move a Spot robot around the scene."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1771364",
   "metadata": {},
   "source": [
    "## Part 1: Reconstructing a Scene with Gaussian Splatting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b53641d",
   "metadata": {},
   "source": [
    "### Gather Images \n",
    "Your reconstruction can only be as good as the data you start with, because of this it's crucial to have good images. For outdoor scene there are some best practices you can follow to get good results. \n",
    "* If you have one main object or area you're focusing on it is recommend to circle or orbit it with a camera equipped aerial vehicle. More details on object centric collection can be found in [nerf_dataset_tips.md]( https://github.com/NVlabs/instant-ngp/blob/master/docs/nerf_dataset_tips.md).\n",
    "* For larger outdoor scenes with no one specific focus either overlapping circular orbits, or traditional oblique mapping flight lines produce good quality results. An example flight mode for a popular drone model can be found [here](https://enterprise-insights.dji.com/blog/smart-oblique-capture).\n",
    "\n",
    "In this lab, we will show an example of an orbit collection using a video captured during flight. Frames are extracted every second or so for training. [FFmpeg](https://www.ffmpeg.org/), an open-source software for audio and video file processing, can be used to save individual frames from a video. Below is an example of how you can save 1 frame per second out as an image from an input video."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2efacd",
   "metadata": {},
   "source": [
    "<table style=\"width:80%;\">\n",
    "  <tr>\n",
    "    <td style=\"width:50%; text-align:center;\">\n",
    "      <img src=\"images/safety_park_camera_poses.png\" alt=\"First Image\" style=\"width:100%;\">\n",
    "      <div>Camera positions for a scene with a single area of focus. Cameras point towards the center of the scene wile orbiting around it. This scene is Safety Park, we will be using this scene for the rest of the lab.</div>\n",
    "    </td>\n",
    "    <td style=\"width:50%; text-align:center;\">\n",
    "      <img src=\"images/civil_air_patrol_camera_poses.png\" alt=\"Second Image\" style=\"width:100%;\">\n",
    "      <div>Camera positions for a large scene with no specific single area of focus. In this case the camera makes many orbits, covering about 7 square kilometers.</div>\n",
    "    </td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49150106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save 1 frame per second from the original video to the **images_raw** folder as images.\n",
    "!ffmpeg -i ../../Data/safety_park/safety_park.webm -vf fps=1 ../../Results/safety_park/images_raw/output_frame_%04d.png"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba65225",
   "metadata": {},
   "source": [
    "For this lab we have curated data for you to use. We will be reconstructing Safety Park, a small pseudo town used for first responder training. Let's view the original video and some of the images of this park that we will use for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39124830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# play video\n",
    "! vlc ../../Data/safety_park/safety_park.webm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006b43d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View images\n",
    "from IPython.display import Image\n",
    "Image(filename=\"../../Data/safety_park/images_raw/000059.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453e29ec-6690-4ff1-928c-eee1146ad248",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename=\"../../Data/safety_park/images_raw/000114.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae6ff48",
   "metadata": {},
   "source": [
    "### Run Structure from Motion (COLMAP)\n",
    "Many radiance field rendering methods, including Gaussian Splatting, require the camera positions and a sparse point cloud of the scene for initialization. We currently have a folder of raw images, but no corresponding camera location or pose information. We can use a structure from motion tool (SfM) to estimate where the camera was for each image and to create a sparse point cloud of the scene. A commonly used SfM tool is [COLMAP](https://colmap.github.io/install.html), which will use in combinations with GLOMAP. [GLOMAP](https://github.com/colmap/glomap) replaces COLMAP's mapper step, focusing on global positioning rather than incremental, and can run 10 or even 100 times faster than COLMAP's. Below we have provided example commands, there's no need to run them here as we have provided the result from an existing SfM run. You can run these commands on your own data by downloading COLMAP and GLOMAP and changing the **/Data/Path** to a directory that contains the **images_raw** folder of your data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81bb62d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example COLMAP & GLOMAP commands\n",
    "\n",
    "# Run the feature extract to identify key points\n",
    "!colmap feature_extractor \\\n",
    "    --database_path /Data/Path/database.db \\\n",
    "    --image_path /Data/Path/images_raw \\\n",
    "    --ImageReader.camera_model PINHOLE \\\n",
    "    --ImageReader.single_camera 1 \\\n",
    "    --SiftExtraction.use_gpu 1 \\\n",
    "    --SiftExtraction.max_image_size 7096 \\\n",
    "    --SiftExtraction.max_num_features 20000 \\\n",
    "    --SiftExtraction.num_threads 14\n",
    "\n",
    "# Mature the features across images\n",
    "!colmap exhaustive_matcher \\\n",
    "    --database_path /Data/Path/database.db \\\n",
    "    --SiftMatching.use_gpu 1 \\\n",
    "    --SiftMatching.max_num_matches 60000 \\\n",
    "    --SiftMatching.guided_matching=true\n",
    "\n",
    "# Create a sparse 3D point cloud of the scene using GLOMAP's global mapper\n",
    "!glomap mapper \\\n",
    "    --database_path /Data/Path/database.db \\\n",
    "    --image_path /Data/Path/images_raw \\\n",
    "    --output_path /Data/Path/sparse \\\n",
    "    --GlobalPositioning.use_gpu 1 \\\n",
    "    --BundleAdjustment.use_gpu 1\n",
    "\n",
    "# Align 3d model by applying transformations\n",
    "!colmap model_aligner \\\n",
    "    --input_path /Data/Path/sparse/0 \\\n",
    "    --output_path /Data/Path/sparse/aligned \\\n",
    "    --database_path /Data/Path/database.db \\\n",
    "    --ref_is_gps 1 \\\n",
    "    --alignment_type ECEF \\\n",
    "    --alignment_max_error 3.0\n",
    "\n",
    "# Undistort original input images so they are as if they were taken with a pinhole camera\n",
    "!colmap image_undistorter \\\n",
    "    --image_path /Data/Path/images_raw \\\n",
    "    --input_path /Data/Path/sparse/0 \\\n",
    "    --output_path /Data/Path \\\n",
    "    --output_type=COLMAP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5de621e",
   "metadata": {},
   "source": [
    "Let's take a closer look at the files from the provided SfM run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e583f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print contents of directory in tree like format\n",
    "!tree ../../Data/safety_park"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfc8b87",
   "metadata": {},
   "source": [
    "Now let's delve deeper into each of these files and folders.\n",
    "\n",
    "**images_raw**: Contains the original images <br>\n",
    "**images**: Contains undistorted images <br>\n",
    "**sparse**: Contains the sparse 3D reconstruction of the scene in folder 0. If COLMAP cannot register the images into 1 single scene, it will be split in to additional numbered folders. <br>\n",
    "**cameras.bin**: Camera intrinsics including camera IDs, camera models, and sensor dimensions. <br>\n",
    "**images.bin**: Camera poses and keypoints for all reconstructed images. <br>\n",
    "**points3D.bin**: The sparse 3D point cloud <br>\n",
    "To learn more these files COLMAP produces see the [COLMAP's Output Format page](https://colmap.github.io/format.html).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60144cc7",
   "metadata": {},
   "source": [
    "### fVDB Reality Capture\n",
    "Now that we have a SfM run we can visualize it and use it to train a Gaussian splatting scene. We will use an [fVDB](https://github.com/openvdb/fvdb-core) example project called [fVDB Reality Capture](https://github.com/openvdb/fvdb-reality-capture) for visualization, manipulation and training. fVDB is a framework for encoding and operating on sparse voxel hierarchies of features in PyTorch. Voxels are like pixels but they are cubes instead of squares, making them three dimensional. Sparse means we only have voxels in areas of our scene that that are occupied, voxels that don't contain anything are not represented. fVDB Reality Capture (fRC) is toolbox for reality capture tasks built on top of Æ’VDB. It gathers the tools required to create an Isaac Sim compatible 3D reconstruction from a set of images and uses fVDB to make the tools fast and efficient.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57c3846",
   "metadata": {},
   "source": [
    "#### Setup and imports\n",
    "\n",
    "Before we get started, let's import the pacakges we need. Here's an overview of some of the important ones:\n",
    "\n",
    "* `logging`\n",
    "    - We'll use the built-in Python `logging` module, and call `logging.basicConfig()` which will cause functions within `fvdb_reality_capture` to log to stdout. You don't have to enable this, but it's useful to see what's happening under the hood.\n",
    "* `fvdb` \n",
    "    - We use `fvdb` for the underlying Gaussian splat data structure (`fvdb.GaussianSplat3d`) which provides fast and scalable rendering, and for interactive visualization (using the `fvdb.viz`) module.\n",
    "* `fvdb_reality_capture` \n",
    "    - We use this for core algorithms that reconstruct scenes from sensors help us read and process capture data (Duh!)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2757a98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "import cv2\n",
    "import fvdb\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import fvdb_reality_capture as frc\n",
    "\n",
    "# Let's use verbose logging to track what happens under the hood.\n",
    "# For less output set level=logging.WARN. For more set level=logging.DEBUG\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(levelname)s : %(message)s\")\n",
    "\n",
    "# Initialize the fvdb.viz module for interactive 3D visualization.\n",
    "# This will spin up a small HTTP server in the background.\n",
    "fvdb.viz.init(port=8080)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f879edf",
   "metadata": {},
   "source": [
    "#### Displaying a Sfm Scene\n",
    "\n",
    "We can use fRC to view and manipulate our SfM scene since it supports loading in capture data stored in different formats into a common representation that can be easily manipulated by users. To do this, data from a capture is stored in an `fvdb_reality_capture.SfmScene` object which acts as an in memory representation of a 3D capture. We will use this object to manipulate and visualize our SfM scene. Let's import the required libraries and load our scene into a `SfmScene`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89251ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Safety Park SfM Scene\n",
    "sfm_scene: frc.SfmScene = frc.SfmScene.from_colmap(\"../../Data/safety_park\")\n",
    "print(\"Loaded SfM Scene\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c17268",
   "metadata": {},
   "source": [
    "Let's take a closer look at what a SfmScene scene consists of.\n",
    "* `SfmScene.cameras`: A dictionary mapping unique camera IDs to SfmCameraMetadata objects which camera parameters (e.g. projection matrices, distortion parameters). The size of this dictionary matches the number of cameras used to capture the scene (so if you scanned a scene with a pair of stereo cameras, then len(SfmScene.cameras) will be 2).\n",
    "* `SfmScene.images`: A list of SfmImageMetadata objects which contain paths to the images and optional masks, a reference to the camera (SfmCameraMetadata) used to capture each image, their camera-to-world (and inverse) transformations, and the set of 3D points visible in each image.\n",
    "* `points/points_rgb/points_err`: Numpy arrays of shape (N,3)/(N,3)/(N,) encoding known surface points in the scene, their RGB colors, and an unnormalized confidence value of the accuracy of that point. Note, N denotes the number of points here. <br>\n",
    "Now that we have a loaded SfmScene, let's plot some of its images, and the projected 3D points within those images.\n",
    "\n",
    "We can use this object to to display our training images and project the 3D points from the sparse point cloud on to them.  We'll visualize the 3D points and cameras interactively using `fvdb.viz` and we'll plot some images with their visible points using `matplotlib`. The `fvdb.viz` module provides a high performance debug visualizer written in [vulkan](https://www.vulkan.org/). It spins up a small HTTP server which streams a visualization to a notebook or a browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46ac46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize an image in an SfmScene and the 3D points visible from that images\n",
    "# projected onto the image plane as blue dots.\n",
    "def plot_image_from_scene(scene: frc.sfm_scene.SfmScene, image_id: int):\n",
    "    image_meta: frc.sfm_scene.SfmPosedImageMetadata = scene.images[image_id]\n",
    "    camera_meta: frc.sfm_scene.SfmCameraMetadata = image_meta.camera_metadata\n",
    "\n",
    "    # Get the visible 3d points for this image\n",
    "    visible_points_3d: np.ndarray = scene.points[image_meta.point_indices]\n",
    "\n",
    "    # Project those points onto the image plane\n",
    "    # 1. Get the world -> camera space transform and projection matrix\n",
    "    world_to_cam_matrix: np.ndarray = image_meta.world_to_camera_matrix\n",
    "    projection_matrix: np.ndarray = camera_meta.projection_matrix\n",
    "    # 2. Transform world points to camera space\n",
    "    visible_points_3d_cam_space = world_to_cam_matrix[:3,:3] @ visible_points_3d.T + world_to_cam_matrix[:3,3:4]\n",
    "    # 3. Transform camera space coordinates to image space\n",
    "    visible_points_2d = projection_matrix @ visible_points_3d_cam_space\n",
    "    visible_points_2d /= visible_points_2d[2]\n",
    "\n",
    "    # Load the image and convert to RGB (OpenCV uses BGR by default)\n",
    "    loaded_image = cv2.imread(image_meta.image_path)\n",
    "    assert loaded_image is not None, f\"Failed to load image at {image_meta.image_path}\"\n",
    "    loaded_image = cv2.cvtColor(loaded_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Plot the image and projected points\n",
    "    plt.title(f\"SfmScene Image {image_id}\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(loaded_image)\n",
    "    plt.scatter(visible_points_2d[0], visible_points_2d[1], color=\"#432de9\", marker=\".\", s=2)\n",
    "\n",
    "# Visualize the SfmScene interactively in a 3D viewer using fvdb.viz.Viewer\n",
    "def visualize_sfm_scene(scene: frc.sfm_scene.SfmScene,\n",
    "                        name: str,\n",
    "                        center_scene: bool = False):\n",
    "\n",
    "    viewer_scene = fvdb.viz.get_scene(\"SfmScene Visualization\")\n",
    "    # Optionally center the scene at the origin.\n",
    "    # This is useful to visualize multiple scenes together without them being far apart.\n",
    "    if center_scene:\n",
    "        center_transform = np.eye(4)\n",
    "        center_transform[:3, 3] = -np.median(scene.points, axis=0)\n",
    "        scene = scene.apply_transformation_matrix(center_transform)\n",
    "\n",
    "    # Plot the points in the SfmScene with their colors (which are uint8 by default but the viewer\n",
    "    # expects float32 colors in [0,1]).\n",
    "    # Each point is drawn as a small sphere with a 2 pixel radius.\n",
    "    viewer_scene.add_point_cloud(\n",
    "        name=f\"{name} Points\",\n",
    "        points=scene.points,\n",
    "        colors=scene.points_rgb.astype(np.float32) / 255.0,\n",
    "        point_size=2.0)\n",
    "\n",
    "    # Plot the cameras as coordinate frames with axis length 2 units,\n",
    "    # and frustums whose distance from the origin to camera plane is 1 unit long.\n",
    "    viewer_scene.add_cameras(\n",
    "        f\"{name} Cameras\",\n",
    "        camera_to_world_matrices=scene.camera_to_world_matrices,\n",
    "        projection_matrices=scene.projection_matrices,\n",
    "        axis_length=2,\n",
    "        frustum_scale=2.5,\n",
    "    )\n",
    "\n",
    "    # Set the initial camera view to be at the position of the first posed image, in the SfmScene,\n",
    "    # looking at the center of the 3D points, with Z as up (COLMAP SfM scenes use Z as up).\n",
    "    viewer_scene.set_camera_lookat(\n",
    "        eye=scene.image_camera_positions[0],\n",
    "        center=np.zeros(3),\n",
    "        up=np.array([0, 0, 1]),  # Z is up in COLMAP SfM scenes\n",
    "    )\n",
    "\n",
    "\n",
    "# Plot three images from the scene and their visible 3D points alongside each other\n",
    "plt.figure(figsize=(25, 4.25))\n",
    "plt.subplot(1, 3, 1)\n",
    "plot_image_from_scene(sfm_scene, 8)\n",
    "plt.subplot(1, 3, 2)\n",
    "plot_image_from_scene(sfm_scene, 16)\n",
    "plt.subplot(1, 3, 3)\n",
    "plot_image_from_scene(sfm_scene, 32)\n",
    "plt.show()\n",
    "\n",
    "# View the SfmScene interactively in a 3D viewer\n",
    "visualize_sfm_scene(sfm_scene, \"Raw SfmScene\", center_scene=True)\n",
    "fvdb.viz.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515e7bc8",
   "metadata": {},
   "source": [
    "If you want to know more about manipulation of SfmScenes in fvdb see the [Reconstructing a Gaussian Splat Radiance Field and High Quality Mesh from a Capture\n",
    "](https://github.com/openvdb/fvdb-reality-capture/blob/main/notebooks/radiance_field_and_mesh_reconstruction.ipynb) notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb6b6d9",
   "metadata": {},
   "source": [
    "#### Train a Gaussian Splatting Scene \n",
    "In this section we will walk through training a Gaussian splatting scene from a set of images and a SfM output, but let's start with what a Gaussian splatting scene is.\n",
    "\n",
    "[3D Gaussian Splatting](https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/) is the dominant radiance field representation. A [radiance field](https://radiancefields.com/) is a 3D scene representation that specifies the color of a point along a given view direction. Radiance fields enable high fidelity visualization of 3D captures with realistic lighting effects. 3D Gaussian splats encode a radiance field function as a sum of 3D Gaussian functions or splats, parameterized by their means (positions), rotations, and scales. Additionally each splat has an opacity value and a set of spherical harmonics that map the direction in which a Gaussian is viewed to a color. Each scene is made up of thousands to millions of these 3-dimensional splats. Think of them as similar to voxels, or points in a point cloud. We start with splats in the same positions as the points in the sparse point cloud we got from our SfM run. During training splats are added, removed, split, and changed until the Gaussian splatting scene is capable of producing renders that are nearly identical to the training images. \n",
    "\n",
    "Now that we have an understanding of what a Gaussian splatting scene is, let's train our own. The core API for Gaussian Splat reconstruction is the `fvdb_reality_capture.GaussianSplatReconstruction` class. It accepts an input `SfmScene` and optional config parameters, and produces an `fvdb.GaussianSplat3d` reconstructing the scene. This functionally is also written in `train.py`. We will show how to use both methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99935e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fvdb_reality_capture.training import Config, SceneOptimizationRunner\n",
    "# Create config so we can edit various parameters\n",
    "cfg = Config();\n",
    "cfg.eval_at_percent = [100]; # save evaluation for the end of training\n",
    "cfg.save_at_percent = [100]; # Don't save the model until the end of training\n",
    "cfg.refine_every_epoch = 4.5; # How often to refine Gaussians during optimization\n",
    "cfg.pose_opt_start_epoch = 100; # Epoch at which we start optimizing camera positions\n",
    "\n",
    "# Create a SceneOptimizationRunner\n",
    "runner = frc.radiance_fields.GaussianSplatReconstruction.from_sfm_scene(\n",
    "        config=cfg, # add the config with the parameters we changed\n",
    "        dataset_path=\"../../Data/safety_park\", # Path to data\n",
    "        results_path=\"../../Results\", # Where to save results\n",
    "        image_downsample_factor=2, # Factor to scale images down by\n",
    "        disable_viewer=True, # Turn off live viewer\n",
    "        normalization_type=\"ecef2enu\", # Normalization type, using Earth-centered Earth-fixed coordinates to local east-north-up\n",
    "        points_percentile_filter= 2, # Amount of points that is too few for an image to contain and still use for training\n",
    "        use_every_n_as_val= -1, # Don't leave any images for validation at the end, use all for training\n",
    "    );\n",
    "\n",
    "# Train\n",
    "runner.optimize();\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20085bc",
   "metadata": {},
   "source": [
    "Higher resolution images and a larger quantity of images will cause training to be longer. We have pretrained splat for you to use. To stop the training, click the square button at the top of the notebook to interrupt the kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19024a12",
   "metadata": {},
   "source": [
    "Our optimizer was created with several different parameters. They effect when we save the model, how often the splats are refined, and more. There are many more parameters you can vary for training, run the training script with the help flag to see them all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b3224f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python train.py -h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8b0baa",
   "metadata": {},
   "source": [
    "Trying changing parameters to see how they effect training. We have added some for you to adjust and build off of. To stop the run early you can click the square button at the top of the notebook to interrupt the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d1d3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try changing and adding parameters\n",
    "!python train.py --dataset-path ../../Data/safety_park --results_path ../../Results --image-downsample-factor 4 --cfg.max-epochs 100 --cfg.eval-at-percent 100 --disable_viewer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20cc81f0",
   "metadata": {},
   "source": [
    "#### Visualize a Gaussian Splatting Scene\n",
    "A major benefit of a 3D radiance fields is that we can render them continuously from any point in space in real time. Let's interactively visualize the reconstructed Gaussian Splat, examining the result from novel, freeform viewpoints. The viewer in `fvdb.viz` makes this straightforward by letting us visualize `fvdb.GaussianSplat3d` objects directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff19829b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the trained splat model from the reconstruction runner\n",
    "model: fvdb.GaussianSplat3d = runner.model\n",
    "# model, metadata = fvdb.GaussianSplat3d.from_ply(\"./reconstructed_model.ply\")\n",
    "\n",
    "# Clear previous contents from the viewer\n",
    "# viewer.clear()\n",
    "\n",
    "# Add our splat model to the viewer\n",
    "scene = fvdb.viz.get_scene(\"Gaussian Splat Model Visualization\")\n",
    "scene.add_gaussian_splat_3d(\"Reconstructed Gaussian Splat Radiance Field\", model)\n",
    "\n",
    "scene.add_cameras(\n",
    "    \"Input Cameras\",\n",
    "    camera_to_world_matrices=cleaned_sfm_scene.camera_to_world_matrices,\n",
    "    projection_matrices=cleaned_sfm_scene.projection_matrices,\n",
    "    axis_length=2,\n",
    "    frustum_scale=2.5,\n",
    ")\n",
    "\n",
    "# Set up the viewer's initial camera to be positioned at the first camera in the SfmScene\n",
    "# looking at the center of the scene. This should give a good initial view of the model.\n",
    "camera_position = cleaned_sfm_scene.images[0].origin\n",
    "camera_lookat_point = model.means.mean(dim=0)\n",
    "scene.set_camera_lookat(eye=camera_position, center=camera_lookat_point, up=(0, 0, 1)) # Colmap uses Z as up\n",
    "fvdb.viz.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e441c7d",
   "metadata": {},
   "source": [
    "### Editing Gaussian Splats\n",
    "[Supersplat](https://github.com/playcanvas/supersplat) is an easy to use web browser based tool for viewing and editing Gaussian splatting scenes. We will use a local version, but in the future you can access it from [superspl.at/editor](https://superspl.at/editor) or locally. Let's launch this tool and start editing our splats. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff111392",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd ../supersplat && npm run develop;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160f2b2e",
   "metadata": {},
   "source": [
    "1. To see the viewer, go to [localhost:3000](http://localhost:3000/) in a web browser. \n",
    "2. In the upper left of the window navigate to to **File > Import**\n",
    "3. In the pop up file explore navigate to **/home/nvidia/Reconstructing_Outdoor_Environments/Data/safety_park_output**, and import the **safety_park_splats.ply**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b1ea3e-47c3-49d0-a6df-10a390b03472",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to see the Supersplat demo video\n",
    "from IPython.display import Video\n",
    "Video(\"./images/Supersplat_demo.webm\", width=800, embed=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8d3489",
   "metadata": {},
   "source": [
    "Now that the Gaussian splatting scene you created is displayed in Supersplat, you can move around, zoom in and out, and edit the splat. Experiment with the tools, such as the transform and selection tools. When your're done, shut down Supersplat by using the square button at the top of the notebook to interrupt the kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab41a03f",
   "metadata": {},
   "source": [
    "### Create Isaac Sim Ready Files\n",
    "The PLY file containing a Gaussian splat scene we get at the end of training is not compatible with Isaac. Let's go over how we can get our scene ready for use in Isaac Sim."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37951e3",
   "metadata": {},
   "source": [
    "#### Convert to a Mesh\n",
    "In order for a robot to move around in Isaac Sim there needs to be a surface it can interact with. Gaussian splatting scenes cannot represent a solid surface in Isaac Sim, so we need to convert our splats to something capable of acting as a surface a robot can collide with. Specifically we will convert the splats into a mesh. Our Gaussian splat model is capable of producing images and depths from novel views. A natural way to reconstruct a mesh from a Gaussian scene is by rendering depth maps from the camera poses used to reconstruct the model and fuse them into a truncated signed distance field (TSDF) using the TSDF fusion algorithm<sup>[\\[2\\]](#references)</sup>. \n",
    "\n",
    "TSDF fusion accumulates noisy depth maps into a voxel grid, to approximate a signed distance field near the surface of the object. `fvdb_reality_capture` makes use of `fvdb-core` to provide a high performance implementation of TSDF integration on sparse voxels. This allows us to generate meshes from Gaussian splats at very high resolutions on the GPU while using minimal memory.\n",
    "\n",
    "The TSDF fusion algorithm is based on the paper: [KinectFusion: Real-Time Dense Surface Mapping and Tracking](https://www.microsoft.com/en-us/research/publication/kinectfusion-real-time-3d-reconstruction-and-interaction-using-a-moving-depth-camera/).\n",
    "The DLNR foundation model is described in the paper: [High-frequency Stereo Matching Network](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhao_High-Frequency_Stereo_Matching_Network_CVPR_2023_paper.pdf).\n",
    "In short, this algorithm works by rendering stereo pairs of images from multiple views of the Gaussian splat model, and using DLNR to compute depth maps from these stereo pairs. The depth maps and images are then integrated into a sparse `fvdb.Grid` in a narrow band around the surface using a weighted averaging scheme. The algorithm returns this grid along with signed distance values and colors (or other features) at each voxel.\n",
    "The mesh can then be extracted from the TSDF using the marching cubes algorithm implemented in `fvdb.marching_cubes.marching_cubes`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db8132b-87c2-406b-b1c6-43a32d364dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "--ply_path ../../Data/safety_park_half/pretrained_splat.ply \\\n",
    "\n",
    "import point_cloud_utils as pcu\n",
    "\n",
    "# The truncation margin determines the width of the narrow band around the surface in which we compute the TSDF.\n",
    "# A larger margin will produce coarser voxels, while a smaller margin will produce finer voxels but may miss some surface details.\n",
    "# Here we pick a truncation margin of 0.25 world units in our scene.\n",
    "truncation_margin = 0.5\n",
    "\n",
    "# This function returns a tensor of vertices, faces, and colors for the mesh.\n",
    "# The vertices have shape (num_vertices, 3), the faces have shape (num_faces, 3),\n",
    "# and the colors have shape (num_vertices, 3). The colors are in the range [0, 1].\n",
    "v, f, c = mesh_from_splats(model, cleaned_sfm_scene.camera_to_world_matrices, cleaned_sfm_scene.projection_matrices, cleaned_sfm_scene.image_sizes, truncation_margin)\n",
    "\n",
    "# Save the mesh as a PLY file for viewing in external tools using point_cloud_utils (https://fwilliams.info/point-cloud-utils/) [3]\n",
    "pcu.save_mesh_vfc(\"../../Results/safety_park_isaac_files/safety_park_mesh.ply\", v.cpu().numpy(), f.cpu().numpy(), c.cpu().numpy())\n",
    "\n",
    "print(f\"Reconstructed mesh with {v.shape[0]:,} vertices and {f.shape[0]:,} faces\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0649f68",
   "metadata": {},
   "source": [
    "#### Cropping and Converting\n",
    "\n",
    "As we saw in Supersplat, splats end up beyond the main focus of our scene. The training process tries to reconstruct the details in all the images, even details far in the horizon. This can lead to messing looking edges. You can remove the edges in Supersplat but we will crop both the mesh and PLY to focus on the center of our scene using **create_isaac_sim_ready_files.py**. Isaac Sim also uses different world axes coordinates than COLMAP, it assumes Z+ is up rather than -Y, so we will rotate the scene accordingly. Additionally, to ensure our mesh will act as a collider in Isaac Sim we will make it water tight. This will fill in holes in the mesh and smooth it out. While we are at it we will convert the mesh from a PLY to an OBJ, a format compatible with Isaac Sim. Isaac Sim doesn't understand a Gaussian splatting scene in PLY format either, so we will have to convert ours into something Isaac Sim can render. We will convert it to a Universal Scene Description Zip (USDZ), this is a compressed version of the USD file type commonly used with Isaac Sim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac446a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the script to make Isaac Sim Ready Files\n",
    "# --input_splat: Location of Gaussian splatting scene\n",
    "# --input_mesh: Location of mesh (PLY)\n",
    "# --output_path: Where to save the OBJ and USDZ (no file extension)\n",
    "# --bbox: Box to crop to\n",
    "# --resolution: How detailed our mesh will be. Increase if you want more faces and vertices.\n",
    "!python scripts/create_isaac_ready_files.py \\\n",
    "--input-splat ../../Data/safety_park_half/pretrained_splat.ply \\\n",
    "--input-mesh ../../Data/safety_park_half/pretrained_mesh.ply \\\n",
    "--output-path ../../Results/safety_park_isaac_files/safety_park_cropped \\\n",
    "--bbox -100 -70 -20 110 90 20 \\\n",
    "--resolution 10000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d574cfe",
   "metadata": {},
   "source": [
    "## Part 2: Creating an Isaac Sim Scene"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a007239",
   "metadata": {},
   "source": [
    "Isaac Sim 5.0 and above includes [NuRec (Neural Reconstruction) rendering](https://docs.isaacsim.omniverse.nvidia.com/5.0.0/assets/usd_assets_nurec.html), adding the functionality to render Gaussian splatting scenes among other neural volume methods. In this section we will go through the process of creating an environment from a Gaussian splatting scene and mesh. Then, using Isaac Lab, we will walk a [Spot](https://bostondynamics.com/products/spot/) robot around the scene. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0edac3",
   "metadata": {},
   "source": [
    "### Running Isaac Sim\n",
    "To run Isaac Sim you need to locate the isaacsim folder, navigate to the release subdirectory, and run the isaac-sim.sh file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8afb81bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd ../isaacsim/_build/linux-x86_64/release && ./isaac-sim.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83139cb0",
   "metadata": {},
   "source": [
    "After a few moments Isaac Sim will open and you should see the following window. \n",
    "\n",
    "<img src=\"./lab_images/Isaac_sim_launch.png\" width=\"800\" style=\"display:block; margin:auto;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86dd3b8",
   "metadata": {},
   "source": [
    "### Import the Assets\n",
    "Once Isaac Sim has launched you can import the Gaussian splatting scene and mesh made in [Part 1](#part-1-creating-a-digital-twin-with-gaussian-splatting).\n",
    "1. In the content tab navigate to **/home/nvidia/Reconstructing_Outdoor_Environments/Data/isaac_files**, inside this folder is the **safety_park_mesh_res_50000.obj**, drag the file into the stage window on the right.\n",
    "2. In the content window, still in **/home/nvidia/Reconstructing_Outdoor_Environments/Data/isaac_files**, drag **safety_park_splats.usdz** into the stage window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc35737-d331-485f-9bbc-3bd3462dd88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to see the Isaac Sim file import demo video\n",
    "from IPython.display import Video\n",
    "Video(\"./images/import_mesh_and_splats.webm\", width=800, embed=True )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262a59d1",
   "metadata": {},
   "source": [
    "Now the 3D Gaussian scene and the mesh should be overlapping in the viewer. The mesh will be very small at first, let's see how we can fix that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa58b013",
   "metadata": {},
   "source": [
    "### Scene Setup\n",
    "Let's setup our scene so its ready for a robot.\n",
    "1. In the **Stage** tab click the **safety_park_mesh_res_50000** xform.\n",
    "2. In the **Property** tab, under **Transform**, change the **Scale:unitsResolve** to 1.0 for X, Y and Z.\n",
    "4. Back in the stage right click the **safety_park_mesh_res_50000** xform and select **Add > Physics > Colliders Preset**\n",
    "    * This makes it so other objects collied with our mesh instead passing right through it.\n",
    "5. Click the eye icon next to the **safety_park_mesh_res_50000** in the **Stage** window to hide the mesh.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a373d891-d389-4241-bba4-57dff7627319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to see the align splats and mesh in Isaac Sim demo video\n",
    "from IPython.display import Video\n",
    "Video(\"./images/scene_setup.webm\", width=800, embed=True )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63db3117",
   "metadata": {},
   "source": [
    "Now the mesh and the 3D Gaussian scene are aligned. We also hid the mesh from view. It will still act as a collider but now we can use just high resolution splats for the visualization of Safety Park."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2fd9cfb",
   "metadata": {},
   "source": [
    "### Optional: Splat Color Editing\n",
    "Currently, lights don't interact with the Gaussian splatting scene, just the mesh. Since changing the strength of lights in the environment will have no effect on the look of the splats, we can artificially change the lighting by changing the emissive color values of the Gaussian splatting scene.\n",
    "1. In the **Stage** tab, select the **safety_park_splat > gauss > gauss > emissive_color_field** asset.\n",
    "2. In the **Property** tab, under **Raw USD Properiteies**\n",
    "    * Change Z in **emissive_color_field.omni:nurec:ccmB** to .7\n",
    "    * Change Y in **emissive_color_field.omni:nurec:ccmG** to .7\n",
    "    * Change X in **emissive_color_field.omni:nurec:ccmR** to .7\n",
    "\n",
    "These values represent the amount the strength of emission for each of the 3 color channels. Be decreasing them all to 0.7 the splat looks less bright. Experiment with changing the values. Changing them non uniformly will result in changes to the color of the scene, rather than just the intensity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2029d364-16d3-4b7f-89e7-7addc2204794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to see the ddjust splat colors in Isaac Sim demo video\n",
    "from IPython.display import Video\n",
    "Video(\"./images/color.webm\", width=800, embed=True )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979b3298",
   "metadata": {},
   "source": [
    "### Save Scene\n",
    "To use the scene we created with Isaac Lab and a Spot robot we need to save it.\n",
    "1. Navigate to **File > Save As...**\n",
    "2. In the file browser that pops up navigate to **/home/nvidia/Reconstructing_Outdoor_Environments/Result/safety_park_isaac_files**\n",
    "3. Save the scene as **isaac_sim_scene.usd**.\n",
    "4. Exit out of Isaac Sim using the close button."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d199a0-e5bb-4920-884e-9db7e1f8e4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to see the save scene as USD demo video\n",
    "from IPython.display import Video\n",
    "Video(\"./images/save_usd.webm\", width=800, embed=True )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3623ba96",
   "metadata": {},
   "source": [
    "### Isaac Lab and Robot Locomotion\n",
    "We can now use Isaac Lab and Isaac Sim to teleoperate a quadruped robot model (Boston Dynamics Spot) around our saved scene using a keyboard or a gaming controller. A locomotion policy was trained for this robot model using reinforcement learning in Isaac Lab, and we can inference this policy to translate velocity commands from the keyboard or controller into the joint-level actions required for the robot to walk. We need to launch Isaac Lab from outside our fVDB Python environment, so lets open a terminal.\n",
    "\n",
    "1. Open a terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3613519e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gnome-terminal --working-directory=/home/nvidia/Reconstructing_Outdoor_Environments/Code/IsaacLab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da807f7d",
   "metadata": {},
   "source": [
    "2. To ensure Isaac Lab has access to as much GPU as possible, lets kill the Jupyter Notebook. Go to **File > Shut Down**. When promoted confirm you want to shut down."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29df6a7",
   "metadata": {},
   "source": [
    "3. Isaac Lab needs to be ran outside of conda Python environments, including the fvdb environment and the base environment. Copy and paste the following command into the terminal so we are no longer working inside a conda environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b681478d-203d-468b-be7d-245449a14fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deactivate conda environments\n",
    "conda deactivate && conda deactivate && conda deactivate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54bec7bc",
   "metadata": {},
   "source": [
    "4. Now we can open our scene with a locomotion policy equipped Spot robot using Isaac Lab. Copy and paste the following command into the terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0827abd6-9af4-4c68-9427-1d0e782fab75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Isaac Lab to launch Isaac Sim with with a Spot robot and location policy in Isaac Sim\n",
    "./isaaclab.sh -p /home/nvidia/Reconstructing_Outdoor_Environments/Code/robo_rl2/policy_inference_in_usd_safetypark.py --checkpoint /home/nvidia/Reconstructing_Outdoor_Environments/Code/robo_rl2/policy.pt --keyboard --terrain_usd /home/nvidia/Reconstructing_Outdoor_Environments/Data/isaac_files/safety_park_isaac_scene.usd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167e38c2",
   "metadata": {},
   "source": [
    "5. Use the arrow keys to move the Spot robot around. You can use the **X** and **Z** to control the yaw and turn the robot. Try running into objects to see how the collider is stopping the Spot.\n",
    "6. The camera should follow the Spot robot around the scene as you explore Safety Park. You can change the position of the camera in the **Isaac Lab** tab on the right. Try changing the **Camera Eye** and **Camera Target** to see how it changes your view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0d16eb-db7c-43d8-91fb-9c56676e9780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to see the Isaac Lab demo video\n",
    "from IPython.display import Video\n",
    "Video(\"./images/isaac_lab.webm\", width=800, embed=True )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd019c8d",
   "metadata": {},
   "source": [
    "Congratulations! You've completed this course, *Reconstructing Outdoor Environments for Physical AI Simulation with 3D Gaussian Splatting in NVIDIA Isaac Sim*. You can now create a 3D Gaussian Splatting reconstuction of a scene and use that scene with NVIDIA Omniverse in Isaac Sim and Isaac Lab."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
