{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6557b8fe",
   "metadata": {},
   "source": [
    "<img src=\"./images/DLI_Header.png\" width=400/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7aaf77d",
   "metadata": {},
   "source": [
    "# Reconstructing Outdoor Environments for Physical AI Simulation with 3D Gaussian Splatting in NVIDIA Isaac Sim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a806ec0",
   "metadata": {},
   "source": [
    "**Table of Contents**\n",
    "<br>\n",
    "In this notebook we demonstrate how to reconstruct an outdoor scene. This includes the following sections:\n",
    "\n",
    "[Part 1: Reconstructing a Scene with Gaussian Splatting](#part-1-reconstructing-a-scene-with-gaussian-splatting)<br>\n",
    "    &emsp;&emsp;1. [Run Structure from Motion (COLMAP)](#run-structure-from-motion-colmap)<br>\n",
    "    &emsp;&emsp;2. [fVDB Reality Capture](#fvdb-reality-capture)<br>\n",
    "    &emsp;&emsp;&emsp;&emsp;a) [Setup and Imports](#setup-and-imports) <br>\n",
    "    &emsp;&emsp;&emsp;&emsp;b) [Preprocess Scene](#preprocess-scene) <br>\n",
    "    &emsp;&emsp;&emsp;&emsp;c) [Train a Gaussian Splatting Scene](#train-a-gaussian-splatting-scene) <br>\n",
    "    &emsp;&emsp;&emsp;&emsp;d) [Convert to a Mesh](#convert-to-a-mesh) <br>\n",
    "    &emsp;&emsp;&emsp;&emsp;e) [Create Isaac Sim Ready Files](#create-isaac-sim-ready-files)<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55ef886-79a4-4e9c-84d1-feaeb6ae1678",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Overview\n",
    "In this lab we will learn to how to reconstruct an outdoor scene to test robots using NVIDIA fVDB framework and NVIDIA Omniverse NuRec rendering in Isaac Sim. We will walk through core reconstruction and rendering technologies, with a step-by-step workflow for simulating an entire outdoor environment for testing any robot. We will learn how to position images using structure from motion, train a 3D Gaussian splatting scene, extract 3D mesh, and convert to USD for simulation in Isaac Sim. This lab is split into two parts. In [Part 1](#part-1-creating-a-digital-twin-with-gaussian-splatting), we will learn how to gather good data, use a SfM tool, and train a Gaussian splatting scene. In [Part 2](#Part-2-creating-an-isaac-sim-scene), we will switch focus to robotic simulation. We will import the files from [Part 1](#part-1-creating-a-digital-twin-with-gaussian-splatting) into Isaac Sim and use Isaac Lab to move a [Spot](https://bostondynamics.com/products/spot/) robot around the scene."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1771364",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Part 1: Reconstructing a Scene with Gaussian Splatting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae6ff48",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Run Structure from Motion (COLMAP)\n",
    "Many radiance field rendering methods, including 3D Gaussian Splatting, require the camera positions and a sparse point cloud of the scene for initialization. We currently have a folder of raw images, but no corresponding camera location or pose information. We can use a structure from motion tool (SfM) to estimate where the camera was for each image and to create a sparse point cloud of the scene. A commonly used SfM tool is [COLMAP](https://colmap.github.io/install.html), which we will use in combination with GLOMAP. [GLOMAP](https://github.com/colmap/glomap) replaces COLMAP's mapper step, focusing on global positioning rather than incremental, and can run 10 or even 100 times faster than COLMAP's. Below we have provided example commands. There's no need to run them here as we have provided the result from an existing SfM run. You can run these commands on your own data by downloading COLMAP and GLOMAP and changing the **/Data/Path** to a directory that contains the **images_raw** folder of your data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81bb62d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example COLMAP & GLOMAP commands\n",
    "\n",
    "# # Run the feature extract to identify key points\n",
    "# !colmap feature_extractor \\\n",
    "#     --database_path /Data/Path/database.db \\\n",
    "#     --image_path /Data/Path/images_raw \\\n",
    "#     --ImageReader.camera_model PINHOLE \\\n",
    "#     --ImageReader.single_camera 1 \\\n",
    "#     --SiftExtraction.use_gpu 1 \\\n",
    "#     --SiftExtraction.max_image_size 7096 \\\n",
    "#     --SiftExtraction.max_num_features 20000 \\\n",
    "#     --SiftExtraction.num_threads 14\n",
    "\n",
    "# # Mature the features across images\n",
    "# !colmap exhaustive_matcher \\\n",
    "#     --database_path /Data/Path/database.db \\\n",
    "#     --SiftMatching.use_gpu 1 \\\n",
    "#     --SiftMatching.max_num_matches 60000 \\\n",
    "#     --SiftMatching.guided_matching=true\n",
    "\n",
    "# # Create a sparse 3D point cloud of the scene using GLOMAP's global mapper\n",
    "# !glomap mapper \\\n",
    "#     --database_path /Data/Path/database.db \\\n",
    "#     --image_path /Data/Path/images_raw \\\n",
    "#     --output_path /Data/Path/sparse \\\n",
    "#     --GlobalPositioning.use_gpu 1 \\\n",
    "#     --BundleAdjustment.use_gpu 1\n",
    "\n",
    "# # Align 3d model by applying transformations\n",
    "# !colmap model_aligner \\\n",
    "#     --input_path /Data/Path/sparse/0 \\\n",
    "#     --output_path /Data/Path/sparse/aligned \\\n",
    "#     --database_path /Data/Path/database.db \\\n",
    "#     --ref_is_gps 1 \\\n",
    "#     --alignment_type ECEF \\\n",
    "#     --alignment_max_error 3.0\n",
    "\n",
    "# # Undistort original input images so they are as if they were taken with a pinhole camera\n",
    "# !colmap image_undistorter \\\n",
    "#     --image_path /Data/Path/images_raw \\\n",
    "#     --input_path /Data/Path/sparse/0 \\\n",
    "#     --output_path /Data/Path \\\n",
    "#     --output_type=COLMAP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5de621e",
   "metadata": {},
   "source": [
    "Let's take a closer look at the files from the provided SfM run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2757a98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "import cv2\n",
    "import fvdb\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import fvdb_reality_capture as frc\n",
    "\n",
    "# Let's use verbose logging to track what happens under the hood.\n",
    "# For less output set level=logging.WARN. For more set level=logging.DEBUG\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(levelname)s : %(message)s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ade514-1ca9-4f12-8f87-62e704340e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download example data for running splats on\n",
    "frc.tools.download_example_data(dataset=\"safety_park\", download_path=\"./data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e583f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print contents of directory in tree like format\n",
    "!tree ./data/safety_park"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfc8b87",
   "metadata": {},
   "source": [
    "Now let's delve deeper into each of these files and folders.\n",
    "\n",
    "**images_raw**: Contains the original images <br>\n",
    "**images**: Contains undistorted images <br>\n",
    "**sparse**: Contains the sparse 3D reconstruction of the scene in folder 0. If COLMAP cannot register the images into 1 single scene, it will be split in to additional numbered folders. <br>\n",
    "**cameras.bin**: Camera intrinsics including camera IDs, camera models, and sensor dimensions. <br>\n",
    "**images.bin**: Camera poses and keypoints for all reconstructed images. <br>\n",
    "**points3D.bin**: The sparse 3D point cloud <br>\n",
    "To learn more these files COLMAP produces see the [COLMAP's Output Format page](https://colmap.github.io/format.html).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60144cc7",
   "metadata": {},
   "source": [
    "### fVDB Reality Capture\n",
    "Now that we have a SfM run we can visualize it and use it to train a Gaussian splatting scene. We will use an [fVDB](https://github.com/openvdb/fvdb-core) example project called [fVDB Reality Capture](https://github.com/openvdb/fvdb-reality-capture) for visualization, manipulation and training. fVDB is a framework for encoding and operating on sparse voxel hierarchies of features in PyTorch. Voxels are like pixels but they are cubes instead of squares, making them three dimensional. Sparse means we only have voxels in areas of our scene that are occupied, voxels that don't contain anything are not represented. fVDB Reality Capture (fRC) is toolbox for reality capture tasks built on top of fVDB. It gathers the tools required to create an Isaac Sim compatible 3D reconstruction from a set of images and uses fVDB to make the tools fast and efficient.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57c3846",
   "metadata": {},
   "source": [
    "#### Setup and Imports\n",
    "\n",
    "Before we get started, let's import the packages we need. Here's an overview of some of the important ones:\n",
    "\n",
    "* `logging`\n",
    "    - We'll use the built-in Python `logging` module, and call `logging.basicConfig()` which will cause functions within `fvdb_reality_capture` to log to stdout. You don't have to enable this, but it's useful to see what's happening under the hood.\n",
    "* `fvdb` \n",
    "    - We use `fvdb` for the underlying Gaussian splat data structure (`fvdb.GaussianSplat3d`) which provides fast and scalable rendering, and for interactive visualization (using the `fvdb.viz`) module.\n",
    "* `fvdb_reality_capture` \n",
    "    - We use this for core algorithms that reconstruct scenes from sensors help us read and process capture data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f879edf",
   "metadata": {},
   "source": [
    "#### Visualize a Sfm Scene\n",
    "\n",
    "We can use fRC to view and manipulate our SfM scene since it supports loading in capture data stored in different formats into a common representation that can be easily manipulated by users. To do this, data from a capture is stored in an `fvdb_reality_capture.SfmScene` object which acts as an in-memory representation of a 3D capture. We will use this object to manipulate and visualize our SfM scene. Let's load our scene into a `SfmScene` by passing in the folder containing all the SfM output and images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89251ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Safety Park SfM Scene\n",
    "sfm_scene = frc.sfm_scene.SfmScene.from_colmap(\"./data/safety_park\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c17268",
   "metadata": {},
   "source": [
    "Let's take a closer look at what a SfmScene scene consists of.\n",
    "* `SfmScene.cameras`: A dictionary mapping unique camera IDs to `SfmCameraMetadata` objects which contain camera parameters (e.g. projection matrices, distortion parameters). The size of this dictionary matches the number of cameras used to capture the scene (so if you scanned a scene with a pair of stereo cameras, then len(SfmScene.cameras) will be 2).\n",
    "* `SfmScene.images`: A list of SfmImageMetadata objects which contain paths to the images and optional masks, a reference to the camera (SfmCameraMetadata) used to capture each image, their camera-to-world (and inverse) transformations, and the set of 3D points visible in each image.\n",
    "* `points/points_rgb/points_err`: Numpy arrays of shape (N,3)/(N,3)/(N,) encoding known surface points in the scene, their RGB colors, and an unnormalized confidence value of the accuracy of that point. Note, N denotes the number of points here. <br>\n",
    "\n",
    "Now that we have a loaded SfmScene, let's plot some of its images, and the projected 3D points within those images. We can use this object to display our training images and project the 3D points from the sparse point cloud on to them.  We'll visualize the 3D points and cameras interactively using `fvdb.viz` and we'll plot some images with their visible points using `matplotlib`. The `fvdb.viz` module provides a high performance debug visualizer written in [vulkan](https://www.vulkan.org/). It spins up a small HTTP server which streams a visualization to a notebook or a browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46ac46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize an image in an SfmScene and the 3D points visible from that images\n",
    "# projected onto the image plane as blue dots.\n",
    "def plot_image_from_scene(scene: frc.sfm_scene.SfmScene, image_id: int):\n",
    "    image_meta: frc.sfm_scene.SfmPosedImageMetadata = scene.images[image_id]\n",
    "    camera_meta: frc.sfm_scene.SfmCameraMetadata = image_meta.camera_metadata\n",
    "\n",
    "    # Get the visible 3d points for this image\n",
    "    visible_points_3d: np.ndarray = scene.points[image_meta.point_indices]\n",
    "\n",
    "    # Project those points onto the image plane\n",
    "    # 1. Get the world -> camera space transform and projection matrix\n",
    "    world_to_cam_matrix: np.ndarray = image_meta.world_to_camera_matrix\n",
    "    projection_matrix: np.ndarray = camera_meta.projection_matrix\n",
    "    # 2. Transform world points to camera space\n",
    "    visible_points_3d_cam_space = world_to_cam_matrix[:3,:3] @ visible_points_3d.T + world_to_cam_matrix[:3,3:4]\n",
    "    # 3. Transform camera space coordinates to image space\n",
    "    visible_points_2d = projection_matrix @ visible_points_3d_cam_space\n",
    "    visible_points_2d /= visible_points_2d[2]\n",
    "\n",
    "    # Load the image and convert to RGB (OpenCV uses BGR by default)\n",
    "    loaded_image = cv2.imread(image_meta.image_path)\n",
    "    assert loaded_image is not None, f\"Failed to load image at {image_meta.image_path}\"\n",
    "    loaded_image = cv2.cvtColor(loaded_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Plot the image and projected points\n",
    "    plt.title(f\"SfmScene Image {image_id}\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(loaded_image)\n",
    "    plt.scatter(visible_points_2d[0], visible_points_2d[1], color=\"#432de9\", marker=\".\", s=2)\n",
    "\n",
    "\n",
    "# Plot three images from the scene and their visible 3D points alongside each other\n",
    "plt.figure(figsize=(25, 4.25))\n",
    "plt.subplot(1, 3, 1)\n",
    "plot_image_from_scene(sfm_scene, 8)\n",
    "plt.subplot(1, 3, 2)\n",
    "plot_image_from_scene(sfm_scene, 16)\n",
    "plt.subplot(1, 3, 3)\n",
    "plot_image_from_scene(sfm_scene, 32)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515e7bc8",
   "metadata": {},
   "source": [
    "You can interact with the above viewer running on [localhost:8000](http://localhost:8000/). For zooming, hold the scroll wheel and drag the mouse up or down. Zoom out enough to see the camera positions represented in green, You can also pan with the right mouse button and rotate with the left mouse button. \n",
    "\n",
    "If you want to know more about manipulation of SfmScenes using fVDB see the [Reconstructing a Gaussian Splat Radiance Field and High Quality Mesh from a Capture\n",
    "](https://github.com/openvdb/fvdb-reality-capture/blob/main/notebooks/radiance_field_and_mesh_reconstruction.ipynb) notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf69bff0-48a3-4b26-ae66-67edda1f24e7",
   "metadata": {},
   "source": [
    "#### Preprocess Scene\n",
    "You may have noticed the scene in the viewer is rotated with respect to the canonical axes. It's also very noisy with a lot of outlier points.\n",
    "This is because the structure-from-motion algorithm which produced this data had a lot of noisy predictions, and predicted points and cameras in a rotated coordinate frame.\n",
    "Before we reconstruct a Gaussian Splat radiance field, let's clean up our input data a bit. We'll apply the following steps:\n",
    "\n",
    " 1. Downsample the images by a factor of 2 to speed up Gaussian Splat optimization (loading big images can be time consuming), \n",
    " 2. Normalize the scene to an east-north-up coordinate system.\n",
    " 3. Remove outlier points below the bottom 3rd and above top 97th percentiles along the X, Y, and Z, axis.\n",
    " 4. Remove any images with fewer than 50 visible points (these images are likely to have bad pose estimates)\n",
    "\n",
    "`fvdb_reality_capture` makes this kind of cleanup easy, efficient, and seamless using the `transforms` module. \n",
    "Transforms are classes which define a transformation of an `SfmScene`. They inherit from `fvdb_reality_capture.BaseTransform`, and their `__call__` operator accepts an `SfmScene` as input and produces a new `SfmScene` as output. Let's look at some code and visualizations and then dive into how this works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab53626-26ba-46fd-82e3-4d020c38fa73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing using transforms\n",
    "import fvdb_reality_capture.transforms as fvtransforms\n",
    "cleanup_and_resize_transform = fvtransforms.Compose(\n",
    "    # Downsample images by factor of 2\n",
    "    fvtransforms.DownsampleImages(image_downsample_factor=2, image_type=\"jpg\", rescaled_jpeg_quality=95),\n",
    "    # Normalize the scene to an est-north-up coordinate system.\n",
    "    fvtransforms.NormalizeScene(normalization_type=\"ecef2enu\"),\n",
    "    # Remove outlier points\n",
    "    fvtransforms.PercentileFilterPoints(percentile_min=3.0, percentile_max=97.0),\n",
    "    # filters outlier points, and removes images with too few points.\n",
    "    fvtransforms.FilterImagesWithLowPoints(min_num_points=50),\n",
    ")\n",
    "\n",
    "# Apply the transforms\n",
    "sfm_scene = cleanup_and_resize_transform(sfm_scene)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb6b6d9",
   "metadata": {},
   "source": [
    "#### Train a Gaussian Splatting Scene \n",
    "In this section we will walk through training a Gaussian splatting scene from a set of images and a SfM output, but let's start with what a Gaussian splatting scene is.\n",
    "\n",
    "[3D Gaussian Splatting](https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/) is the dominant radiance field representation. A [radiance field](https://radiancefields.com/) is a 3D scene representation that specifies the color of a point along a given view direction. Radiance fields enable high fidelity visualization of 3D captures with realistic lighting effects. 3D Gaussian splats encode a radiance field function as a sum of 3D Gaussian functions or splats, parameterized by their means (positions), rotations, and scales. Additionally each splat has an opacity value and a set of spherical harmonics that map the direction in which a Gaussian is viewed to a color. Each scene is made up of thousands to millions of these splats. Think of them as similar to voxels, or points in a point cloud. We start with splats in the same positions as the points in the sparse point cloud from our SfM run. During training splats are added, removed, split, and changed until the Gaussian splatting scene is capable of producing renders that are nearly identical to the training images. \n",
    "<div>\n",
    "<img src=\"https://fvdb-data.s3.us-east-2.amazonaws.com/fvdb-reality-capture/doc_figures/fvdb_gs_opt.jpg\" width=\"80%\"/>\n",
    "</div>\n",
    "\n",
    "Now that we have an understanding of what a Gaussian splatting scene is, let's train our own. The core API for Gaussian Splat reconstruction is the `fvdb_reality_capture.GaussianSplatReconstruction` class. It accepts an input `SfmScene` and optional config parameters, and produces an `fvdb.GaussianSplat3d` reconstructing the scene. This functionally is also written in `/fvdb_reality_capture/cli/frgs/_reconstruct.py`. We will show how to use both methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ef9b0d-e596-4ca7-b768-5e2426936e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create config so we can edit various parameters\n",
    "cfg = frc.radiance_fields.GaussianSplatReconstructionConfig()\n",
    "cfg.eval_at_percent = [100]  # save evaluation for the end of training\n",
    "cfg.save_at_percent = [100]  # Don't save the model until the end of training\n",
    "cfg.refine_every_epoch = 4.5  # How often to refine Gaussians during optimization\n",
    "cfg.pose_opt_start_epoch = 100  # Epoch at which we start optimizing camera positions\n",
    "\n",
    "# Create a GaussianSplatReconstruction runner\n",
    "runner = frc.radiance_fields.GaussianSplatReconstruction.from_sfm_scene(\n",
    "    config=cfg,  # The reconstruction config\n",
    "    sfm_scene=sfm_scene,  # The SfM scene\n",
    "    use_every_n_as_val=-1,  # Don't leave any images for validation\n",
    ")\n",
    "\n",
    "# Train\n",
    "runner.optimize()\n",
    "\n",
    "# Save results\n",
    "runner.save_ply(\"reconstruction.ply\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20085bc",
   "metadata": {},
   "source": [
    "Higher resolution images and a larger quantity of images will cause training to take longer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19024a12",
   "metadata": {},
   "source": [
    "Our optimizer was created with several different parameters. They effect when we save the model, how often the splats are refined, and more. There are many more parameters you can vary for training beyond these few. We will use the command-line interface (CLI) tool called `frgs` for easy usage of Gaussian splatting tools. To learn more about `frgs` please see [Reconstruction on the CLI with frgs\n",
    "](https://openvdb.github.io/fvdb-reality-capture/tutorials/frgs.html) documentation. Now we'll run the reconstruct script with the help flag to see all the possible arguments and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b3224f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!frgs reconstruct --help"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8b0baa",
   "metadata": {},
   "source": [
    "Trying changing and adding parameters to see how they effect training. To stop the run early you can click the square button at the top of the notebook to interrupt the kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26833e2-2bcb-47de-aa8f-bc36760e4b68",
   "metadata": {},
   "source": [
    "#### Convert to a Mesh\n",
    "In order for a robot to move around in Isaac Sim there needs to be a surface it can interact with. Gaussian splatting scenes cannot represent a solid surface in Isaac Sim, so we need to convert our splats to a mesh capable of acting as a collider. `fvdb_reality_capture.tools` provides `mesh_from_splats_dlnr` which will use a foundation model to compute high quality depth maps from images rendered from our Gaussian splat. \n",
    "The method works by rendering stereo pairs from the splat scene, and uses the [DLNR](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhao_High-Frequency_Stereo_Matching_Network_CVPR_2023_paper.pdf) foundation model to perform stereo depth estimation. The DLNR model is a high-frequency stereo matching network that computes optical flow and disparity maps between two images.\n",
    "The we can fuse the depth estimations into a truncated signed distance field (TSDF) using the [TSDF](https://www.microsoft.com/en-us/research/publication/kinectfusion-real-time-3d-reconstruction-and-interaction-using-a-moving-depth-camera/) fusion algorithm. TSDF fusion accumulates noisy depth maps into a voxel grid, to approximate a signed distance field near the surface of the object. `fvdb_reality_capture` makes use of `fvdb-core` to provide a high performance implementation of TSDF integration on sparse voxels. A mesh can then be extracted from the TSDF using the marching cubes algorithm implemented in `fvdb.marching_cubes.marching_cubes`. This allows us to generate meshes from Gaussian splats at very high resolutions on the GPU while using minimal memory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db8132b-87c2-406b-b1c6-43a32d364dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fvdb_reality_capture.tools import mesh_from_splats\n",
    "import point_cloud_utils as pcu\n",
    "\n",
    "# The truncation margin determines the width of the narrow band around the surface in which we compute the TSDF.\n",
    "# A larger margin will produce coarser voxels, while a smaller margin will produce finer voxels but may miss some surface details.\n",
    "# Here we pick a truncation margin of 0.25 world units in our scene.\n",
    "truncation_margin = 0.5\n",
    "\n",
    "# load the Gaussian splatting scene\n",
    "pretrained_splat_path = \"reconstruction.ply\"\n",
    "model, metadata = fvdb.GaussianSplat3d.from_ply(pretrained_splat_path, device=\"cuda\")\n",
    "# This function returns a tensor of vertices, faces, and colors for the mesh.\n",
    "# The vertices have shape (num_vertices, 3), the faces have shape (num_faces, 3),\n",
    "# and the colors have shape (num_vertices, 3). The colors are in the range [0, 1].\n",
    "v, f, c = mesh_from_splats(model, sfm_scene.camera_to_world_matrices, sfm_scene.projection_matrices, sfm_scene.image_sizes, truncation_margin)\n",
    "\n",
    "# Save the mesh as a PLY file for viewing in external tools using point_cloud_utils (https://fwilliams.info/point-cloud-utils/) [3]\n",
    "pcu.save_mesh_vfc(\"safety_park_mesh.ply\", v.cpu().numpy(), f.cpu().numpy(), c.cpu().numpy())\n",
    "\n",
    "print(f\"Reconstructed mesh with {v.shape[0]:,} vertices and {f.shape[0]:,} faces\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0649f68",
   "metadata": {},
   "source": [
    "#### Create Isaac Sim Ready Files\n",
    "\n",
    "As we saw in viewer, splats end up beyond the main focus of our scene. The training process tries to reconstruct the all the details in the images, even details far in the horizon. This can lead to messy looking edges. We will crop both the mesh and splats to focus on the center of our scene using **create_isaac_sim_ready_files.py**. There are also a few other things this script does to make our mesh and splats compatible with Isaac Sim.\n",
    "\n",
    "* Coordinate System: Isaac Sim uses different world axes coordinates than COLMAP, it assumes Z+ is up rather than -Y, so we will rotate the scene accordingly. \n",
    "* Water Tight: To ensure our mesh will act as a collider in Isaac Sim we will make it water tight. This will fill in holes in the mesh and smooth it out.\n",
    "* Format Mesh: We will convert the mesh from a PLY to an OBJ, a file type compatible Isaac Sim.\n",
    "* Format Splats: Isaac Sim doesn't understand a Gaussian splatting scene in PLY format, so we will have to convert ours into something Isaac Sim can render. USDs are commonly used with Isaac Sim, so we will convert our scene to a Universal Scene Description Zip (USDZ), a compressed version of USD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac446a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the script to make Isaac Sim Ready Files\n",
    "# --input_splat: Location of Gaussian splatting scene\n",
    "# --input_mesh: Location of mesh (PLY)\n",
    "# --output_path: Where to save the OBJ and USDZ (no file extension)\n",
    "# --bbox: Box to crop to\n",
    "# --resolution: How detailed our mesh will be. Increase if you want more faces and vertices.\n",
    "!python ../scripts/create_isaac_ready_files.py \\\n",
    "--input-splat reconstruction.ply \\\n",
    "--input-mesh safety_park_mesh.ply \\\n",
    "--output-path safety_park_cropped \\\n",
    "--bbox -100 -70 -20 110 90 20 \\\n",
    "--resolution 10000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de12742-334a-4d97-8a57-35db1ed8eaad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
